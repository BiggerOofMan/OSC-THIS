"""
Demo script showcasing LLM ingredient research with Meta Llama-3.3-70B
"""

import os
from this_processor import THISProcessor

def demo_llm_research():
    """Demonstrate LLM-powered ingredient research"""
    
    print("ü§ñ THIS + LLM Demo - Unknown Ingredient Research")
    print("=" * 60)
    print()
    
    # Check for LLM configuration
    llm_config_status = []
    if os.getenv('OLLAMA_HOST'):
        llm_config_status.append("‚úÖ Ollama configured")
    if os.getenv('TOGETHER_API_KEY'):
        llm_config_status.append("‚úÖ Together AI configured")
    if os.getenv('OPENAI_API_KEY'):
        llm_config_status.append("‚úÖ OpenAI configured")
    
    if llm_config_status:
        print("LLM Configuration:")
        for status in llm_config_status:
            print(f"  {status}")
    else:
        print("‚ö†Ô∏è  No LLM configuration found. Using static database only.")
        print("   To enable AI research, see config.env.example")
    
    print("\n" + "=" * 60)
    
    # Initialize processor
    processor = THISProcessor()
    
    # Test ingredients with some unknown ones
    test_ingredients = """
    Ingredients: Water, High Fructose Corn Syrup, Citric Acid, Natural Flavors, 
    Sodium Benzoate (Preservative), Caffeine, Glucuronolactone, Aspartame 
    (Contains Phenylalanine), Yellow 5, Methylparaben, Propylparaben, 
    Carrageenan, Xanthan Gum, Polysorbate 80
    """
    
    print(f"üìù Analyzing ingredients with potential LLM research:")
    print(f"Input: {test_ingredients.strip()}")
    print("\n" + "-" * 60)
    
    # Analyze
    try:
        results = processor.analyze_text_directly(test_ingredients, user_allergies=["milk"])
        
        # Display basic results
        print(f"\nüìä Analysis Results:")
        print(f"Health Score: {results['analysis']['health_score']}/10")
        print(f"Total Ingredients: {results['analysis']['total_ingredients']}")
        print(f"Known Ingredients: {results['analysis']['known_ingredients']}")
        
        # LLM Research Summary
        llm_info = results['llm_research_info']
        print(f"\nü§ñ LLM Research Summary:")
        print(f"Research Enabled: {'Yes' if llm_info['research_enabled'] else 'No'}")
        print(f"Ingredients Researched: {llm_info['total_researched']}")
        print(f"High Confidence Results: {llm_info['high_confidence_results']}")
        print(f"Low Confidence Results: {llm_info['low_confidence_results']}")
        
        # Show researched ingredients details
        if llm_info['researched_ingredients']:
            print(f"\nüîç AI-Researched Ingredients:")
            for ingredient in llm_info['researched_ingredients']:
                confidence_emoji = "‚úÖ" if ingredient['confidence'] >= 0.7 else "‚ö†Ô∏è"
                print(f"  {confidence_emoji} {ingredient['name']}")
                print(f"     Purpose: {ingredient['purpose']}")
                print(f"     Safety: {ingredient['safety_level'].title()}")
                print(f"     Natural: {'Yes' if ingredient['natural'] else 'No'}")
                print(f"     Confidence: {ingredient['confidence']:.2f}")
                print()
        
        # Show detailed ingredient info for first few
        print(f"\nüìã Detailed Ingredient Information:")
        for i, ingredient in enumerate(results['analysis']['ingredient_details'][:5]):
            is_llm_research = ingredient['description'].startswith('LLM Research')
            source_emoji = "ü§ñ" if is_llm_research else "üìö"
            
            print(f"\n  {source_emoji} {ingredient['name']}")
            print(f"     Description: {ingredient['description'][:100]}...")
            print(f"     Purpose: {ingredient['purpose']}")
            print(f"     Health Concern: {ingredient['health_concern'].title()}")
            print(f"     Natural: {'Yes' if ingredient['natural'] else 'No'}")
            if ingredient['allergens']:
                print(f"     Allergens: {', '.join(ingredient['allergens'])}")
        
        # Summary and recommendations
        print(f"\nüìù Summary:")
        print(f"{results['summary']}")
        
        print(f"\nüí° Recommendations:")
        for rec in results['recommendations']:
            print(f"  ‚Ä¢ {rec}")
        
        print(f"\n‚è±Ô∏è  Processing time: {results['processing_time_seconds']:.2f} seconds")
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        return False
    
    return True

def setup_instructions():
    """Show setup instructions for different LLM providers"""
    
    print("\n" + "=" * 60)
    print("üõ†Ô∏è  LLM Setup Instructions")
    print("=" * 60)
    
    print("""
üìã To enable AI-powered ingredient research with Llama-3.3-70B:

1Ô∏è‚É£  OLLAMA (Local - Best for privacy):
   ‚Ä¢ Install Ollama: https://ollama.ai/
   ‚Ä¢ Run: ollama pull llama3.3:70b
   ‚Ä¢ Set environment: OLLAMA_HOST=http://localhost:11434
   ‚Ä¢ Pros: Free, private, works offline
   ‚Ä¢ Cons: Requires ~40GB disk space, slower on CPU

2Ô∏è‚É£  TOGETHER AI (Cloud - Best for performance):
   ‚Ä¢ Sign up: https://api.together.xyz/
   ‚Ä¢ Get API key from dashboard
   ‚Ä¢ Set environment: TOGETHER_API_KEY=your_key_here
   ‚Ä¢ Pros: Fast, no local resources needed
   ‚Ä¢ Cons: Costs money, requires internet

3Ô∏è‚É£  OTHER PROVIDERS:
   ‚Ä¢ Many providers support Llama-3.3-70B via OpenAI-compatible APIs
   ‚Ä¢ Set: LLM_API_KEY and LLM_BASE_URL
   ‚Ä¢ Examples: Groq, Anyscale, Fireworks AI

4Ô∏è‚É£  FALLBACK (OpenAI):
   ‚Ä¢ Uses GPT-3.5-Turbo if no Llama access
   ‚Ä¢ Set: OPENAI_API_KEY=sk-your_key_here
   ‚Ä¢ Pros: Reliable, fast
   ‚Ä¢ Cons: Not Llama-3.3-70B, costs money

üí° Create a .env file with your chosen configuration!
""")

if __name__ == "__main__":
    # Run demo
    success = demo_llm_research()
    
    if not success:
        setup_instructions()
    
    print("\n" + "=" * 60)
    print("üéØ Demo complete! Check the results above.")
    print("üí° Tip: Run with LLM configured for best results!")